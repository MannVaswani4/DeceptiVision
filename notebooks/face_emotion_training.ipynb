{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafd929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from src.models.face_cnn import FaceCNN\n",
    "from src.video_to_frames import extract_frames\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465804ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\"../data/raw/fer2013/train\", transform=transform)\n",
    "test_data  = datasets.ImageFolder(\"../data/raw/fer2013/test\",  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_data,  batch_size=32, shuffle=False)\n",
    "\n",
    "emotion_labels = train_data.classes\n",
    "print(\"Classes:\", emotion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb22840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.6836\n",
      "Epoch 2 | Loss: 1.4630\n",
      "Epoch 3 | Loss: 1.3391\n",
      "Epoch 4 | Loss: 1.2449\n",
      "Epoch 5 | Loss: 1.1667\n",
      "Epoch 6 | Loss: 1.0902\n",
      "Epoch 7 | Loss: 1.0210\n",
      "Epoch 8 | Loss: 0.9435\n",
      "Epoch 9 | Loss: 0.8777\n",
      "Epoch 10 | Loss: 0.8062\n",
      "✅ Emotion CNN model saved!\n"
     ]
    }
   ],
   "source": [
    "model = FaceCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss_sum/len(train_loader):.4f}\")\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"../models/emotion_cnn.pth\")\n",
    "print(\"✅ Emotion CNN model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143a345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Emotion CNN loaded for inference\n"
     ]
    }
   ],
   "source": [
    "model = FaceCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"../models/emotion_cnn.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"✅ Emotion CNN loaded for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2389ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(frame_path):\n",
    "    img = Image.open(frame_path).convert(\"L\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(img)\n",
    "        probs = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca4b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.body_language_yolo import extract_yolo_pose\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "NUM_EMOTION_FEATURES = 23\n",
    "NUM_BODY_FEATURES = 7\n",
    "TOTAL_FEATURES = NUM_EMOTION_FEATURES + NUM_BODY_FEATURES   # = 30\n",
    "\n",
    "def process_video_to_features(video_path, class_label, return_features=False, fps=2):\n",
    "    \"\"\"\n",
    "    Extract exactly 30 features per video:\n",
    "        - 23 emotion features\n",
    "        - 7 body-language features\n",
    "    \"\"\"\n",
    "    video_name = Path(video_path).stem\n",
    "    frame_dir = Path(f\"../data/frames/{video_name}\")\n",
    "    frame_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    extract_frames(str(video_path), str(frame_dir), fps=fps)\n",
    "\n",
    "    frames = sorted(f for f in os.listdir(frame_dir) if f.endswith(\".jpg\"))\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        print(f\"⚠️ No frames extracted: {video_name}\")\n",
    "        return None\n",
    "\n",
    "    emotion_list = []\n",
    "    pose_list = []\n",
    "\n",
    "    for f in frames:\n",
    "        fp = str(frame_dir / f)\n",
    "\n",
    "        # Emotion\n",
    "        emot = predict_emotion(fp)\n",
    "        emotion_list.append(emot)\n",
    "\n",
    "        # Pose\n",
    "        pose = extract_yolo_pose(fp)\n",
    "        if pose is None:\n",
    "            pose = np.zeros(51)   # fail-safe\n",
    "        pose_list.append(pose)\n",
    "\n",
    "    emotion_timeline = np.array(emotion_list)  # (N,7)\n",
    "    pose_timeline    = np.array(pose_list)     # (N,51)\n",
    "\n",
    "    # ---------- Emotion Features ----------\n",
    "    dominant = np.argmax(emotion_timeline, axis=1)\n",
    "    ratios = {e: 0 for e in emotion_labels}\n",
    "\n",
    "    unique, counts = np.unique(dominant, return_counts=True)\n",
    "    for idx, c in zip(unique, counts):\n",
    "        ratios[emotion_labels[idx]] = c / len(dominant)\n",
    "\n",
    "    transitions = int(np.sum(dominant[:-1] != dominant[1:]))\n",
    "    volatility = float(np.mean(np.abs(np.diff(emotion_timeline, axis=0))))\n",
    "\n",
    "    peaks = emotion_timeline.max(axis=0)\n",
    "    vars_ = emotion_timeline.var(axis=0)\n",
    "\n",
    "    emotion_features = (\n",
    "        [float(ratios[e]) for e in emotion_labels] +\n",
    "        [float(transitions), float(volatility)] +\n",
    "        [float(v) for v in peaks] +\n",
    "        [float(v) for v in vars_]\n",
    "    )\n",
    "\n",
    "    # Ensure 23 features\n",
    "    emotion_features = emotion_features[:23]\n",
    "\n",
    "    # ---------- Body Language Features ----------\n",
    "    movement = np.linalg.norm(np.diff(pose_timeline, axis=0), axis=1)\n",
    "    movement_mean = float(movement.mean())\n",
    "    movement_var  = float(movement.var())\n",
    "    movement_max  = float(movement.max())\n",
    "\n",
    "    left_shoulder  = pose_timeline[:, 5*3:5*3+2]\n",
    "    right_shoulder = pose_timeline[:, 6*3:6*3+2]\n",
    "    shoulder_var   = float(np.linalg.norm(left_shoulder - right_shoulder, axis=1).var())\n",
    "\n",
    "    nose = pose_timeline[:, 0:3]\n",
    "    head_speed = np.linalg.norm(np.diff(nose, axis=0), axis=1)\n",
    "    head_var = float(head_speed.var())\n",
    "\n",
    "    left_wrist  = pose_timeline[:, 9*3:9*3+3]\n",
    "    right_wrist = pose_timeline[:, 10*3:10*3+3]\n",
    "    nose3 = pose_timeline[:, 0:3]\n",
    "\n",
    "    lw = np.linalg.norm(left_wrist - nose3, axis=1)\n",
    "    rw = np.linalg.norm(right_wrist - nose3, axis=1)\n",
    "\n",
    "    hand_face_min  = float(min(lw.min(), rw.min()))\n",
    "    hand_face_mean = float((lw.mean() + rw.mean()) / 2)\n",
    "\n",
    "    body_features = [\n",
    "        movement_mean, movement_var, movement_max,\n",
    "        shoulder_var, head_var,\n",
    "        hand_face_min, hand_face_mean\n",
    "    ]\n",
    "\n",
    "    features = emotion_features + body_features   # ALWAYS 30\n",
    "\n",
    "    if return_features:\n",
    "        return np.array(features), class_label\n",
    "\n",
    "    return np.array(features), class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c3cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train truth: 48\n",
      "Train lie: 48\n",
      "Test truth: 12\n",
      "Test lie: 13\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "truth = sorted(glob.glob(\"../videos/truth/*.mp4\"))\n",
    "lie   = sorted(glob.glob(\"../videos/lie/*.mp4\"))\n",
    "\n",
    "train_truth, test_truth = train_test_split(truth, test_size=0.2, random_state=42)\n",
    "train_lie,   test_lie   = train_test_split(lie,   test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train truth:\", len(train_truth))\n",
    "print(\"Train lie:\", len(train_lie))\n",
    "print(\"Test truth:\", len(test_truth))\n",
    "print(\"Test lie:\", len(test_lie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f7be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 78 frames from ../videos/truth/trial_truth_032.mp4 → ../data/frames/trial_truth_032\n",
      "✅ Extracted 175 frames from ../videos/truth/trial_truth_004.mp4 → ../data/frames/trial_truth_004\n",
      "✅ Extracted 63 frames from ../videos/truth/trial_truth_053.mp4 → ../data/frames/trial_truth_053\n",
      "✅ Extracted 15 frames from ../videos/truth/trial_truth_018.mp4 → ../data/frames/trial_truth_018\n",
      "✅ Extracted 49 frames from ../videos/truth/trial_truth_009.mp4 → ../data/frames/trial_truth_009\n",
      "✅ Extracted 154 frames from ../videos/truth/trial_truth_007.mp4 → ../data/frames/trial_truth_007\n",
      "✅ Extracted 68 frames from ../videos/truth/trial_truth_041.mp4 → ../data/frames/trial_truth_041\n",
      "✅ Extracted 75 frames from ../videos/truth/trial_truth_005.mp4 → ../data/frames/trial_truth_005\n",
      "✅ Extracted 42 frames from ../videos/truth/trial_truth_044.mp4 → ../data/frames/trial_truth_044\n",
      "✅ Extracted 15 frames from ../videos/truth/trial_truth_020.mp4 → ../data/frames/trial_truth_020\n",
      "✅ Extracted 63 frames from ../videos/truth/trial_truth_035.mp4 → ../data/frames/trial_truth_035\n",
      "✅ Extracted 58 frames from ../videos/truth/trial_truth_059.mp4 → ../data/frames/trial_truth_059\n",
      "✅ Extracted 69 frames from ../videos/truth/trial_truth_026.mp4 → ../data/frames/trial_truth_026\n",
      "✅ Extracted 82 frames from ../videos/truth/trial_truth_057.mp4 → ../data/frames/trial_truth_057\n",
      "✅ Extracted 16 frames from ../videos/truth/trial_truth_016.mp4 → ../data/frames/trial_truth_016\n",
      "✅ Extracted 38 frames from ../videos/truth/trial_truth_028.mp4 → ../data/frames/trial_truth_028\n",
      "✅ Extracted 155 frames from ../videos/truth/trial_truth_010.mp4 → ../data/frames/trial_truth_010\n",
      "✅ Extracted 38 frames from ../videos/truth/trial_truth_031.mp4 → ../data/frames/trial_truth_031\n",
      "✅ Extracted 54 frames from ../videos/truth/trial_truth_027.mp4 → ../data/frames/trial_truth_027\n",
      "✅ Extracted 10 frames from ../videos/truth/trial_truth_017.mp4 → ../data/frames/trial_truth_017\n",
      "✅ Extracted 73 frames from ../videos/truth/trial_truth_025.mp4 → ../data/frames/trial_truth_025\n",
      "✅ Extracted 73 frames from ../videos/truth/trial_truth_056.mp4 → ../data/frames/trial_truth_056\n",
      "✅ Extracted 61 frames from ../videos/truth/trial_truth_012.mp4 → ../data/frames/trial_truth_012\n",
      "✅ Extracted 36 frames from ../videos/truth/trial_truth_033.mp4 → ../data/frames/trial_truth_033\n",
      "✅ Extracted 52 frames from ../videos/truth/trial_truth_054.mp4 → ../data/frames/trial_truth_054\n",
      "✅ Extracted 60 frames from ../videos/truth/trial_truth_042.mp4 → ../data/frames/trial_truth_042\n",
      "✅ Extracted 40 frames from ../videos/truth/trial_truth_038.mp4 → ../data/frames/trial_truth_038\n",
      "✅ Extracted 115 frames from ../videos/truth/trial_truth_030.mp4 → ../data/frames/trial_truth_030\n",
      "✅ Extracted 66 frames from ../videos/truth/trial_truth_045.mp4 → ../data/frames/trial_truth_045\n",
      "✅ Extracted 43 frames from ../videos/truth/trial_truth_002.mp4 → ../data/frames/trial_truth_002\n",
      "✅ Extracted 62 frames from ../videos/truth/trial_truth_022.mp4 → ../data/frames/trial_truth_022\n",
      "✅ Extracted 28 frames from ../videos/truth/trial_truth_003.mp4 → ../data/frames/trial_truth_003\n",
      "✅ Extracted 52 frames from ../videos/truth/trial_truth_048.mp4 → ../data/frames/trial_truth_048\n",
      "✅ Extracted 65 frames from ../videos/truth/trial_truth_040.mp4 → ../data/frames/trial_truth_040\n",
      "✅ Extracted 78 frames from ../videos/truth/trial_truth_036.mp4 → ../data/frames/trial_truth_036\n",
      "✅ Extracted 48 frames from ../videos/truth/trial_truth_024.mp4 → ../data/frames/trial_truth_024\n",
      "✅ Extracted 46 frames from ../videos/truth/trial_truth_050.mp4 → ../data/frames/trial_truth_050\n",
      "✅ Extracted 88 frames from ../videos/truth/trial_truth_011.mp4 → ../data/frames/trial_truth_011\n",
      "✅ Extracted 49 frames from ../videos/truth/trial_truth_023.mp4 → ../data/frames/trial_truth_023\n",
      "✅ Extracted 28 frames from ../videos/truth/trial_truth_019.mp4 → ../data/frames/trial_truth_019\n",
      "✅ Extracted 42 frames from ../videos/truth/trial_truth_060.mp4 → ../data/frames/trial_truth_060\n",
      "✅ Extracted 22 frames from ../videos/truth/trial_truth_021.mp4 → ../data/frames/trial_truth_021\n",
      "✅ Extracted 95 frames from ../videos/truth/trial_truth_008.mp4 → ../data/frames/trial_truth_008\n",
      "✅ Extracted 44 frames from ../videos/truth/trial_truth_043.mp4 → ../data/frames/trial_truth_043\n",
      "✅ Extracted 77 frames from ../videos/truth/trial_truth_015.mp4 → ../data/frames/trial_truth_015\n",
      "✅ Extracted 48 frames from ../videos/truth/trial_truth_029.mp4 → ../data/frames/trial_truth_029\n",
      "✅ Extracted 58 frames from ../videos/truth/trial_truth_052.mp4 → ../data/frames/trial_truth_052\n",
      "✅ Extracted 67 frames from ../videos/truth/trial_truth_039.mp4 → ../data/frames/trial_truth_039\n",
      "✅ Extracted 25 frames from ../videos/lie/trial_lie_004.mp4 → ../data/frames/trial_lie_004\n",
      "✅ Extracted 40 frames from ../videos/lie/trial_lie_054.mp4 → ../data/frames/trial_lie_054\n",
      "✅ Extracted 74 frames from ../videos/lie/trial_lie_018.mp4 → ../data/frames/trial_lie_018\n",
      "✅ Extracted 45 frames from ../videos/lie/trial_lie_009.mp4 → ../data/frames/trial_lie_009\n",
      "✅ Extracted 101 frames from ../videos/lie/trial_lie_007.mp4 → ../data/frames/trial_lie_007\n",
      "✅ Extracted 48 frames from ../videos/lie/trial_lie_042.mp4 → ../data/frames/trial_lie_042\n",
      "✅ Extracted 115 frames from ../videos/lie/trial_lie_005.mp4 → ../data/frames/trial_lie_005\n",
      "✅ Extracted 41 frames from ../videos/lie/trial_lie_045.mp4 → ../data/frames/trial_lie_045\n",
      "✅ Extracted 24 frames from ../videos/lie/trial_lie_020.mp4 → ../data/frames/trial_lie_020\n",
      "✅ Extracted 103 frames from ../videos/lie/trial_lie_048.mp4 → ../data/frames/trial_lie_048\n",
      "✅ Extracted 72 frames from ../videos/lie/trial_lie_053.mp4 → ../data/frames/trial_lie_053\n",
      "✅ Extracted 62 frames from ../videos/lie/trial_lie_026.mp4 → ../data/frames/trial_lie_026\n",
      "✅ Extracted 48 frames from ../videos/lie/trial_lie_058.mp4 → ../data/frames/trial_lie_058\n",
      "✅ Extracted 80 frames from ../videos/lie/trial_lie_016.mp4 → ../data/frames/trial_lie_016\n",
      "✅ Extracted 86 frames from ../videos/lie/trial_lie_059.mp4 → ../data/frames/trial_lie_059\n",
      "✅ Extracted 52 frames from ../videos/lie/trial_lie_028.mp4 → ../data/frames/trial_lie_028\n",
      "✅ Extracted 63 frames from ../videos/lie/trial_lie_010.mp4 → ../data/frames/trial_lie_010\n",
      "✅ Extracted 56 frames from ../videos/lie/trial_lie_031.mp4 → ../data/frames/trial_lie_031\n",
      "✅ Extracted 54 frames from ../videos/lie/trial_lie_037.mp4 → ../data/frames/trial_lie_037\n",
      "✅ Extracted 58 frames from ../videos/lie/trial_lie_027.mp4 → ../data/frames/trial_lie_027\n",
      "✅ Extracted 64 frames from ../videos/lie/trial_lie_025.mp4 → ../data/frames/trial_lie_025\n",
      "✅ Extracted 20 frames from ../videos/lie/trial_lie_044.mp4 → ../data/frames/trial_lie_044\n",
      "✅ Extracted 17 frames from ../videos/lie/trial_lie_012.mp4 → ../data/frames/trial_lie_012\n",
      "✅ Extracted 94 frames from ../videos/lie/trial_lie_033.mp4 → ../data/frames/trial_lie_033\n",
      "✅ Extracted 64 frames from ../videos/lie/trial_lie_055.mp4 → ../data/frames/trial_lie_055\n",
      "✅ Extracted 41 frames from ../videos/lie/trial_lie_057.mp4 → ../data/frames/trial_lie_057\n",
      "✅ Extracted 48 frames from ../videos/lie/trial_lie_038.mp4 → ../data/frames/trial_lie_038\n",
      "✅ Extracted 81 frames from ../videos/lie/trial_lie_030.mp4 → ../data/frames/trial_lie_030\n",
      "✅ Extracted 71 frames from ../videos/lie/trial_lie_046.mp4 → ../data/frames/trial_lie_046\n",
      "✅ Extracted 134 frames from ../videos/lie/trial_lie_002.mp4 → ../data/frames/trial_lie_002\n",
      "✅ Extracted 89 frames from ../videos/lie/trial_lie_022.mp4 → ../data/frames/trial_lie_022\n",
      "✅ Extracted 16 frames from ../videos/lie/trial_lie_003.mp4 → ../data/frames/trial_lie_003\n",
      "✅ Extracted 50 frames from ../videos/lie/trial_lie_049.mp4 → ../data/frames/trial_lie_049\n",
      "✅ Extracted 50 frames from ../videos/lie/trial_lie_040.mp4 → ../data/frames/trial_lie_040\n",
      "✅ Extracted 84 frames from ../videos/lie/trial_lie_036.mp4 → ../data/frames/trial_lie_036\n",
      "✅ Extracted 53 frames from ../videos/lie/trial_lie_024.mp4 → ../data/frames/trial_lie_024\n",
      "✅ Extracted 14 frames from ../videos/lie/trial_lie_051.mp4 → ../data/frames/trial_lie_051\n",
      "✅ Extracted 73 frames from ../videos/lie/trial_lie_011.mp4 → ../data/frames/trial_lie_011\n",
      "✅ Extracted 93 frames from ../videos/lie/trial_lie_023.mp4 → ../data/frames/trial_lie_023\n",
      "✅ Extracted 79 frames from ../videos/lie/trial_lie_019.mp4 → ../data/frames/trial_lie_019\n",
      "✅ Extracted 63 frames from ../videos/lie/trial_lie_061.mp4 → ../data/frames/trial_lie_061\n",
      "✅ Extracted 39 frames from ../videos/lie/trial_lie_021.mp4 → ../data/frames/trial_lie_021\n",
      "✅ Extracted 16 frames from ../videos/lie/trial_lie_008.mp4 → ../data/frames/trial_lie_008\n",
      "✅ Extracted 29 frames from ../videos/lie/trial_lie_043.mp4 → ../data/frames/trial_lie_043\n",
      "✅ Extracted 74 frames from ../videos/lie/trial_lie_015.mp4 → ../data/frames/trial_lie_015\n",
      "✅ Extracted 43 frames from ../videos/lie/trial_lie_029.mp4 → ../data/frames/trial_lie_029\n",
      "✅ Extracted 96 frames from ../videos/lie/trial_lie_052.mp4 → ../data/frames/trial_lie_052\n",
      "✅ Extracted 63 frames from ../videos/lie/trial_lie_039.mp4 → ../data/frames/trial_lie_039\n"
     ]
    }
   ],
   "source": [
    "train_csv = \"../data/deception_train.csv\"\n",
    "\n",
    "with open(train_csv, \"w\") as f:\n",
    "    csv.writer(f).writerow([f\"f{i}\" for i in range(30)] + [\"label\"])\n",
    "\n",
    "for v in train_truth:\n",
    "    feats, lab = process_video_to_features(v, 1, return_features=True)\n",
    "    with open(train_csv, \"a\") as f:\n",
    "        csv.writer(f).writerow(list(feats) + [lab])\n",
    "\n",
    "for v in train_lie:\n",
    "    feats, lab = process_video_to_features(v, 0, return_features=True)\n",
    "    with open(train_csv, \"a\") as f:\n",
    "        csv.writer(f).writerow(list(feats) + [lab])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3ef288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 30 frames from ../videos/truth/trial_truth_001.mp4 → ../data/frames/trial_truth_001\n",
      "✅ Extracted 66 frames from ../videos/truth/trial_truth_006.mp4 → ../data/frames/trial_truth_006\n",
      "✅ Extracted 46 frames from ../videos/truth/trial_truth_037.mp4 → ../data/frames/trial_truth_037\n",
      "✅ Extracted 74 frames from ../videos/truth/trial_truth_046.mp4 → ../data/frames/trial_truth_046\n",
      "✅ Extracted 28 frames from ../videos/truth/trial_truth_014.mp4 → ../data/frames/trial_truth_014\n",
      "✅ Extracted 63 frames from ../videos/truth/trial_truth_055.mp4 → ../data/frames/trial_truth_055\n",
      "✅ Extracted 65 frames from ../videos/truth/trial_truth_034.mp4 → ../data/frames/trial_truth_034\n",
      "✅ Extracted 60 frames from ../videos/truth/trial_truth_049.mp4 → ../data/frames/trial_truth_049\n",
      "✅ Extracted 60 frames from ../videos/truth/trial_truth_013.mp4 → ../data/frames/trial_truth_013\n",
      "✅ Extracted 45 frames from ../videos/truth/trial_truth_058.mp4 → ../data/frames/trial_truth_058\n",
      "✅ Extracted 52 frames from ../videos/truth/trial_truth_047.mp4 → ../data/frames/trial_truth_047\n",
      "✅ Extracted 63 frames from ../videos/truth/trial_truth_051.mp4 → ../data/frames/trial_truth_051\n",
      "✅ Extracted 37 frames from ../videos/lie/trial_lie_001.mp4 → ../data/frames/trial_lie_001\n",
      "✅ Extracted 39 frames from ../videos/lie/trial_lie_006.mp4 → ../data/frames/trial_lie_006\n",
      "✅ Extracted 35 frames from ../videos/lie/trial_lie_047.mp4 → ../data/frames/trial_lie_047\n",
      "✅ Extracted 59 frames from ../videos/lie/trial_lie_032.mp4 → ../data/frames/trial_lie_032\n",
      "✅ Extracted 30 frames from ../videos/lie/trial_lie_014.mp4 → ../data/frames/trial_lie_014\n",
      "✅ Extracted 63 frames from ../videos/lie/trial_lie_056.mp4 → ../data/frames/trial_lie_056\n",
      "✅ Extracted 70 frames from ../videos/lie/trial_lie_035.mp4 → ../data/frames/trial_lie_035\n",
      "✅ Extracted 50 frames from ../videos/lie/trial_lie_050.mp4 → ../data/frames/trial_lie_050\n",
      "✅ Extracted 41 frames from ../videos/lie/trial_lie_013.mp4 → ../data/frames/trial_lie_013\n",
      "✅ Extracted 52 frames from ../videos/lie/trial_lie_041.mp4 → ../data/frames/trial_lie_041\n",
      "✅ Extracted 63 frames from ../videos/lie/trial_lie_034.mp4 → ../data/frames/trial_lie_034\n",
      "✅ Extracted 54 frames from ../videos/lie/trial_lie_060.mp4 → ../data/frames/trial_lie_060\n",
      "✅ Extracted 94 frames from ../videos/lie/trial_lie_017.mp4 → ../data/frames/trial_lie_017\n"
     ]
    }
   ],
   "source": [
    "test_csv = \"../data/deception_test.csv\"\n",
    "\n",
    "with open(test_csv, \"w\") as f:\n",
    "    csv.writer(f).writerow([f\"f{i}\" for i in range(30)] + [\"label\"])\n",
    "\n",
    "for v in test_truth:\n",
    "    feats, lab = process_video_to_features(v, 1, return_features=True)\n",
    "    with open(test_csv, \"a\") as f:\n",
    "        csv.writer(f).writerow(list(feats) + [lab])\n",
    "\n",
    "for v in test_lie:\n",
    "    feats, lab = process_video_to_features(v, 0, return_features=True)\n",
    "    with open(test_csv, \"a\") as f:\n",
    "        csv.writer(f).writerow(list(feats) + [lab])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abac1ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.72\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74        13\n",
      "           1       0.73      0.67      0.70        12\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.72      0.72      0.72        25\n",
      "weighted avg       0.72      0.72      0.72        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"../data/deception_train.csv\")\n",
    "test_df  = pd.read_csv(\"../data/deception_test.csv\")\n",
    "\n",
    "X_train, y_train = train_df.drop(\"label\", axis=1), train_df[\"label\"]\n",
    "X_test,  y_test  = test_df.drop(\"label\", axis=1),  test_df[\"label\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21c22551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classifier saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, \"../models/deception_classifier.pkl\")\n",
    "print(\"✅ Classifier saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3487da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 88 frames from ../videos/truth/sample_test_video.wmv → ../data/frames/sample_test_video\n",
      "Prediction: TRUTH\n"
     ]
    }
   ],
   "source": [
    "X_sample, _ = process_video_to_features(\n",
    "    \"../videos/truth/sample_test_video.wmv\",\n",
    "    class_label=1,\n",
    "    return_features=True\n",
    ")\n",
    "\n",
    "clf = joblib.load(\"../models/deception_classifier.pkl\")\n",
    "\n",
    "import pandas as pd\n",
    "X_input = pd.DataFrame([X_sample], columns=clf.feature_names_in_)\n",
    "\n",
    "prediction = clf.predict(X_input)[0]\n",
    "\n",
    "print(\"Prediction:\", \"TRUTH\" if prediction == 1 else \"LIE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
