{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fafd929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from src.models.face_cnn import FaceCNN\n",
    "from src.video_to_frames import extract_frames\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "465804ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\"../data/raw/fer2013/train\", transform=transform)\n",
    "test_data  = datasets.ImageFolder(\"../data/raw/fer2013/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_data,  batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", train_data.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb22840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.6935\n",
      "Epoch 2 | Loss: 1.4555\n",
      "Epoch 3 | Loss: 1.3263\n",
      "Epoch 4 | Loss: 1.2478\n",
      "Epoch 5 | Loss: 1.1782\n",
      "Epoch 6 | Loss: 1.1170\n",
      "Epoch 7 | Loss: 1.0583\n",
      "Epoch 8 | Loss: 0.9998\n",
      "Epoch 9 | Loss: 0.9388\n",
      "Epoch 10 | Loss: 0.8854\n",
      "✅ Model saved!\n"
     ]
    }
   ],
   "source": [
    "model = FaceCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss_sum/len(train_loader):.4f}\")\n",
    "\n",
    "# Save model ONCE\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"../models/emotion_cnn.pth\")\n",
    "print(\"✅ Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "143a345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded for inference\n"
     ]
    }
   ],
   "source": [
    "model = FaceCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"../models/emotion_cnn.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"✅ Model loaded for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd2389ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(frame_path):\n",
    "    img = Image.open(frame_path)\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(img)\n",
    "        probs = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2c3cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 15 frames to ../data/frames/test1\n",
      "Timeline shape: (15, 7)\n"
     ]
    }
   ],
   "source": [
    "extract_frames(\"../interview.mp4\", \"../data/frames/test1\", fps=2)\n",
    "\n",
    "frames = sorted(os.listdir(\"../data/frames/test1\"))\n",
    "timeline = np.array([predict_emotion(f\"../data/frames/test1/{f}\") for f in frames])\n",
    "\n",
    "print(\"Timeline shape:\", timeline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdee50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (23,)\n"
     ]
    }
   ],
   "source": [
    "labels = ['angry','disgust','fear','happy','neutral','sad','surprise']\n",
    "dominant = np.argmax(timeline, 1)\n",
    "unique, counts = np.unique(dominant, return_counts=True)\n",
    "ratio = dict(zip([labels[u] for u in unique], counts/len(dominant)))\n",
    "\n",
    "transitions = np.sum(dominant[:-1] != dominant[1:])\n",
    "volatility  = np.mean(np.abs(np.diff(timeline, axis=0)))\n",
    "peak   = dict(zip(labels, timeline.max(0)))\n",
    "var    = dict(zip(labels, timeline.var(0)))\n",
    "\n",
    "features = []\n",
    "for e in labels: features.append(ratio.get(e,0))\n",
    "features += [transitions, volatility]\n",
    "for e in labels: features.append(peak[e])\n",
    "for e in labels: features.append(var[e])\n",
    "\n",
    "X_sample = np.array(features, dtype=np.float32)\n",
    "print(\"Feature shape:\", X_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf269ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved to dataset\n"
     ]
    }
   ],
   "source": [
    "label = 0  # change per video\n",
    "\n",
    "row = list(X_sample) + [label]\n",
    "file = \"../data/deception_dataset.csv\"\n",
    "header = [f\"f{i}\" for i in range(23)] + [\"label\"]\n",
    "\n",
    "if not os.path.exists(file):\n",
    "    with open(file,\"w\") as f: csv.writer(f).writerow(header)\n",
    "\n",
    "with open(file,\"a\") as f: csv.writer(f).writerow(row)\n",
    "print(\"✅ saved to dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
